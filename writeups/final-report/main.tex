\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}       % include graphics

% TODO change title
\title{Untitled Project}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Bryan Learn\\
  10-701\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213\\
  \And
  Mike Tasota\\
  10-701\\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213\\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle


\begin{abstract}
  % TODO add real abstract text
  A brief overview of our project.
\end{abstract}


\section{Introduction}

% TODO taken from Proposal, revise text

Although similar to text generation in many respects, there is still significant progress to be made in the area of music generation. There are a few distinctions between the problem of music generation and text generation that affect this. The number of potential states that a standard 88-key piano can take greatly exceeds the number of words in any language. Another difference that is present in music generation is the concept of tempo, which adds even more complexity to the number of representations that generated music can take in comparison to generated text.

Previous rule-based models of generation are now being outperformed thanks to the advent of deep learning techniques. One research effort that has made significant progress is the Magenta research project, which was started by researchers and engineers from the Google Brain team. This package utilizes the TensorFlow library to implement various deep learning techniques for the problem of music generation.


\section{Related Work}

% TODO taken from Proposal, revise text

In the MidiNet paper, Yang et al. created a CNN-GAN based model for MIDI generation [4]. They propose that their model could be possibly expanded to generate multiple tracks. This was demonstrated in the work done by MuseGAN. However, as pointed out in the MusicVAE paper, MuseGAN works with a fixed set of instruments, namely bass, drums, guitar, piano, and strings [3]. Instead, MusicVAE is capable of modeling exactly three broad classes of instruments which it calls the “trio:” melody, bass, and drums. This provides more flexibility on what instruments can be assigned in track generation. However, it also introduces chord conditioning to keep harmony fixed.

Another related work in multiple track generation is from Chu et al. in their “Song from PI” RNN. Although PI is capable of producing multiple tracks, they also conditioned the generation on scale types to help pick up regularities in their dataset of pop songs [1]. In our work, we hope to produce multiple tracks without relying on prior knowledge of music theory in the model.


\section{Methods}

% TODO add methods

% - describe what work you have completed towards creating a method or pipeline which improves on the baseline.
% - What is your motivation behind these techniques
%   --(you are highly encouraged to come up with an original idea of your own rather than simply implementing or applying an existing ML algorithm)
% - Use math/figure to explain your approach, when possible (do not use a lot of space on page)


\section{Dataset}

% TODO elaborate on why classical piano would not suffice (not enough tracks to train on and lack of consistent voices)
% TODO mention comparison on dataset size? (classical piano seems much smaller than NES-MDB)
% TODO mention other artifacts in MIDI files that are being ignored (all the Control_c events)

For our work, it was necessary to have access to a dataset of MIDI files which had consistent track types across the entire dataset. To accomplish this, we used the NES Music Database (NES-MDB) as presented by Donahue et al. [2]. The dataset consists of 5,278 songs from the soundtracks of 397 NES games. Each MIDI file consists of four instrument voices, each mapping to an NES synthesizer. These voices are two pulse-wave generators (P1, P2), a triangle-wave generator (TR), and a percussive noise generator (NO). Since these voices are consistent throughout all files in the dataset, we assumed that each voice represented a specific track type. For example, the percurssive noise generator would be a drum track for all MIDI files in which it is defined.


\section{Experiments and Results}

% TODO add experiments/results

% - Show plots of the performance of your algorithms and interpret what they mean
%   --Be sure to label and explain this clearly
% - What do the results imply about your methods?
% - How did you set up your experiments?
% - What are your baselines?
% - What metrics did you use for evaluation?
%   --How do your results compare to prior work?
% - Describe how the current results in each of the experiments align with your expectations


\section{Conclusion and Future Work}

% TODO add conclusion

% - what you accomplished and what would be the future direction for this project
% - Analyze your model and results
% - Highlight a few limitations of your approach (assumptions made, pitfalls, caveats, etc.)
% - Comment on whether you think there is a way to further improve your method to eliminate these limitations

\section{Acknowledgements}
We thank our project mentor, Jing Mao, for assistance.\\

This work used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

\section*{References}

% TODO needs updated for final report references

\small

[1] Chu, Hang, et al. ``Song From PI: A Musically Plausible Network for Pop Music Generation.'' {\it [1611.03477] Song From PI: A Musically Plausible Network for Pop Music Generation,} 10 Nov. 2016, arxiv.org/abs/1611.03477.

[2] Donahue, Chris et al. ``The NES Music Database: A multi-instrumental dataset with expressive performance attributes'' {\it [1806.04278] The NES Music Database: A multi-instrumental dataset with expressive performance attributes,} 12 June 2018, arxiv.org/abs/1806.04278.

[3] Simon, Ian, et al. ``Learning a Latent Space of Multitrack Measures.'' {\it[1806.00195] Learning a Latent Space of Multitrack Measures,} 1 June 2018, arxiv.org/abs/1806.00195.

[4] Yang, Li-Chia, et al. ``MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation.'' {\it[1703.10847] MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation,} 18 July 2017, arxiv.org/abs/1703.10847.

\end{document}

